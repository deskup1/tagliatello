connections:
  Inference LLM_2.message -> Display Text 0.in:
    input_name: in
    input_node_name: Display Text 0
    output_name: message
    output_node_name: Inference LLM_2
  Llm Chat Message 0.message -> Inference LLM_2.message:
    input_name: message
    input_node_name: Inference LLM_2
    output_name: message
    output_node_name: Llm Chat Message 0
  Ollama Chat 0.model -> Inference LLM_2.model:
    input_name: model
    input_node_name: Inference LLM_2
    output_name: model
    output_node_name: Ollama Chat 0
nodes:
  Display Text 0:
    default_inputs: {}
    default_outputs: {}
    metadata:
      position:
      - 824
      - 587
    static_inputs: {}
    type: Display Text
  Inference LLM_2:
    default_inputs:
      frequency_penalty: -1.0
      max_tokens: 200
      message: null
      messages: []
      stop: []
      temperature: -1.0
      top_p: -1.0
    default_outputs: {}
    metadata:
      position:
      - 392
      - 427
    static_inputs: {}
    type: Inference LLM
  Llm Chat Message 0:
    default_inputs:
      image: screenshots/screenshot1.png
      message: Describe this image
      role: user
    default_outputs: {}
    metadata:
      position:
      - -235
      - 607
    static_inputs: {}
    type: Llm Chat Message
  Ollama Chat 0:
    default_inputs: {}
    default_outputs: {}
    metadata:
      position:
      - -122
      - 301
    static_inputs:
      api_key: <DUMMY_KEY>
      base_url: http://localhost:11434
      model: internlm/internlm2.5:7b-chat
      timeout: 60.0
    type: Ollama Chat
