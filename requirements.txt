
# torch linux
https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp312-cp312-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.12"
https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp311-cp311-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.11"
https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp310-cp310-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.10"
https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp39-cp39-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.9"
https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp38-cp38-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.8"

# torch windows
https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp312-cp312-win_amd64.whl; platform_system == "Windows" and python_version == "3.12"
https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp311-cp311-win_amd64.whl; platform_system == "Windows" and python_version == "3.11"
https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp310-cp310-win_amd64.whl; platform_system == "Windows" and python_version == "3.10"
https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp39-cp39-win_amd64.whl; platform_system == "Windows" and python_version == "3.9"
https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp38-cp38-win_amd64.whl; platform_system == "Windows" and python_version == "3.8"

torch==2.3.1

# FlashAttention (Florence-2) linux
https://github.com/oobabooga/flash-attention/releases/download/v2.5.9.post1/flash_attn-2.5.9.post1+cu118torch2.3cxx11abiFALSE-cp38-cp38-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.8"
https://github.com/oobabooga/flash-attention/releases/download/v2.5.9.post1/flash_attn-2.5.9.post1+cu118torch2.3cxx11abiFALSE-cp39-cp39-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.9"
https://github.com/oobabooga/flash-attention/releases/download/v2.5.9.post1/flash_attn-2.5.9.post1+cu118torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.10"
https://github.com/oobabooga/flash-attention/releases/download/v2.5.9.post1/flash_attn-2.5.9.post1+cu118torch2.3cxx11abiFALSE-cp311-cp311-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.11"
https://github.com/oobabooga/flash-attention/releases/download/v2.5.9.post1/flash_attn-2.5.9.post1+cu118torch2.3cxx11abiFALSE-cp312-cp312-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.12"

# FlashAttention (Florence-2) windows
https://github.com/oobabooga/flash-attention/releases/download/v2.5.9.post1/flash_attn-2.5.9.post1+cu122torch2.3.1cxx11abiFALSE-cp311-cp311-win_amd64.whl; platform_system == "Windows" and python_version == "3.11"
https://github.com/oobabooga/flash-attention/releases/download/v2.5.9.post1/flash_attn-2.5.9.post1+cu122torch2.3.1cxx11abiFALSE-cp311-cp311-win_amd64.whl; platform_system == "Windows" and python_version == "3.10"
# flash-attn==2.5.9.post1

numpy==1.26.4

accelerate
pandas
filetype
dearpygui
opencv-python
pillow
pyyaml
huggingface_hub
onnxruntime-gpu
transformers
openai
packaging
timm


